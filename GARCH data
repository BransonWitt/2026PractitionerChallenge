import numpy as np
import pandas as pd
from scipy.stats import t as t_dist
from arch import arch_model
import yfinance as yf
import time
import os
import json
import hashlib

# =============================================================================
# REPRODUCIBILITY SETTINGS
# =============================================================================

MASTER_SEED = 42  # Change this to generate different datasets

def set_all_seeds(seed):
    """Set all random seeds for reproducibility"""
    np.random.seed(seed)
    print(f"All seeds set to: {seed}")

def compute_dataframe_hash(df, sample_size=10000):
    """
    Compute hash of dataframe to verify reproducibility.
    Uses a sample to avoid memory issues with large datasets.
    """
    if len(df) > sample_size:
        df_sample = df.sample(n=sample_size, random_state=42).sort_index()
    else:
        df_sample = df
    
    return hashlib.md5(pd.util.hash_pandas_object(df_sample).values).hexdigest()

def save_metadata(metadata, filename='dataset_metadata.json'):
    """Save generation metadata for verification"""
    with open(filename, 'w') as f:
        json.dump(metadata, f, indent=2)
    print(f"\nMetadata saved to {filename}")

# =============================================================================
# STEP 1: DOWNLOAD FTSE DATA AND FIT GARCH
# =============================================================================

print("=" * 70)
print("STEP 1: DOWNLOADING FTSE DATA AND FITTING GARCH MODEL")
print("=" * 70)

# Set seed before any random operations
set_all_seeds(MASTER_SEED)

data = yf.download("^FTSE", start="2019-01-01", end="2025-01-01")

if isinstance(data.columns, pd.MultiIndex):
    close_prices = data['Close']['^FTSE']
else:
    close_prices = data['Close']

S0 = float(close_prices.iloc[-1])

print(f"Downloaded {len(close_prices):,} trading days")
print(f"S_0 (initial stock price): £{S0:,.2f}")

# Fit GARCH(1,1)-t
returns_pct = 100 * np.log(close_prices / close_prices.shift(1)).dropna()

print("\nFitting GARCH(1,1)-t model...")
model = arch_model(returns_pct, vol='Garch', p=1, q=1, dist='t')
result = model.fit(disp='off')

mu = result.params['mu']
omega = result.params['omega']
alpha = result.params['alpha[1]']
beta = result.params['beta[1]']
nu = result.params['nu']

print(f"Parameters: μ={mu:.4f}%, ω={omega:.6f}, α={alpha:.4f}, β={beta:.4f}, ν={nu:.2f}")

# =============================================================================
# STEP 2: PARAMETERS
# =============================================================================

print("\n" + "=" * 70)
print("STEP 2: SIMULATION PARAMETERS")
print("=" * 70)

n_simulations = 100000
T_maturity = 5  # All simulations start at 5 years
K_percentages = np.array([0.60, 0.70, 0.80, 0.90, 1.00, 1.10, 1.20, 1.30, 1.40, 1.50])
days_per_year = 252
n_days = T_maturity * days_per_year  # Total days for each simulation
mu_adjusted = 0.04

print(f"Total simulations: {n_simulations:,}")
print(f"Maturity (T): {T_maturity} years for ALL simulations")
print(f"Days per simulation: {n_days}")
print(f"K choices: {K_percentages * 100}%")

# Store metadata for reproducibility
metadata = {
    'master_seed': MASTER_SEED,
    'n_simulations': int(n_simulations),
    'T_maturity': int(T_maturity),
    'K_percentages': K_percentages.tolist(),
    'S0': float(S0),
    'days_per_year': int(days_per_year),
    'n_days': int(n_days),
    'mu_adjusted': float(mu_adjusted),
    'garch_params': {
        'mu': float(mu),
        'omega': float(omega),
        'alpha': float(alpha),
        'beta': float(beta),
        'nu': float(nu)
    },
    'data_download': {
        'ticker': '^FTSE',
        'start_date': '2019-01-01',
        'end_date': '2025-01-01',
        'n_days': int(len(close_prices))
    },
    'python_version': os.sys.version,
    'numpy_version': np.__version__,
    'pandas_version': pd.__version__,
    'generation_timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
}

# =============================================================================
# STEP 3: ASSIGN K STRIKES (WITH SEED CONTROL)
# =============================================================================

print("\n" + "=" * 70)
print("STEP 3: ASSIGNING STRIKE PRICES")
print("=" * 70)

# CRITICAL: Set seed before random assignments
np.random.seed(MASTER_SEED)

# Each simulation gets ONE randomly assigned strike price
K_pct_assignments = np.random.choice(K_percentages, size=n_simulations)
K_assignments = S0 * K_pct_assignments

print(f"K distribution:")
for k_pct in K_percentages:
    count = np.sum(K_pct_assignments == k_pct)
    print(f"  K={k_pct*100:.0f}%: {count:,} ({count/n_simulations*100:.1f}%)")

# =============================================================================
# STEP 4: VECTORIZED GARCH SIMULATION FUNCTION
# =============================================================================

def simulate_garch_vectorized(S0, n_days, n_scenarios, mu, omega, alpha, beta, nu, seed=None):
    """
    Fully vectorized GARCH(1,1)-t simulation with seed control.
    
    Args:
        seed: If provided, sets numpy seed before simulation
    """
    if seed is not None:
        np.random.seed(seed)
    
    var0 = omega / (1 - alpha - beta)
    
    S_paths = np.zeros((n_scenarios, n_days))
    sigma_paths = np.zeros((n_scenarios, n_days))
    
    S_paths[:, 0] = S0
    sigma_paths[:, 0] = np.sqrt(var0)
    
    var_current = np.full(n_scenarios, var0)
    S_current = np.full(n_scenarios, S0)
    
    for day in range(1, n_days):
        z = t_dist.rvs(df=nu, size=n_scenarios) / np.sqrt(nu / (nu - 2))
        
        sigma_current = np.sqrt(var_current)
        eps = sigma_current * z
        return_pct = mu + eps
        
        S_current = S_current * np.exp(return_pct / 100)
        
        S_paths[:, day] = S_current
        sigma_paths[:, day] = sigma_current
        
        var_current = omega + alpha * (eps ** 2) + beta * var_current
        var_current = np.minimum(var_current, 25.0)
    
    return S_paths, sigma_paths

# =============================================================================
# STEP 5: RUN SIMULATIONS (VECTORIZED WITH SEED CONTROL)
# =============================================================================

print("\n" + "=" * 70)
print("STEP 4: RUNNING SIMULATIONS (VECTORIZED)")
print("=" * 70)

output_dir = 'garch_simulation_data'
os.makedirs(output_dir, exist_ok=True)

total_start = time.time()

print(f"\nGenerating {n_simulations:,} simulations × {n_days} days")

# --- VECTORIZED GARCH SIMULATION WITH SEED ---
print(f"Running GARCH simulation...")
sim_start = time.time()

# CRITICAL: Use deterministic seed
simulation_seed = MASTER_SEED + 5000

S_paths, sigma_paths = simulate_garch_vectorized(
    S0=S0,
    n_days=n_days,
    n_scenarios=n_simulations,
    mu=mu_adjusted,
    omega=omega,
    alpha=alpha,
    beta=beta,
    nu=nu,
    seed=simulation_seed
)

print(f"GARCH simulation completed in {time.time() - sim_start:.1f}s")

# --- BUILD DATASET (VECTORIZED) ---
print(f"\nBuilding dataset...")
build_start = time.time()

n_rows = n_simulations * n_days

# Create arrays for each column
simulation_col = np.repeat(np.arange(n_simulations), n_days)
day_col = np.tile(np.arange(n_days), n_simulations)
S_col = S_paths.flatten()
K_col = np.repeat(K_assignments, n_days)

# Create T column: starts at T_maturity and decreases each day
T_sequence = T_maturity - np.arange(n_days) / days_per_year
T_col = np.tile(T_sequence, n_simulations)

# Convert volatility to annualized percentage
sigma_col = sigma_paths.flatten() * np.sqrt(days_per_year) / 100

print(f"Arrays built in {time.time() - build_start:.1f}s")

# --- CREATE DATAFRAME ---
print(f"Creating DataFrame...")
df_start = time.time()

df = pd.DataFrame({
    'simulation': simulation_col,
    'day': day_col,
    'S': S_col,
    'K': K_col,
    'T': T_col,
    'sigma': sigma_col
})

# Sort by simulation, then descending T (day ascending)
df = df.sort_values(
    by=['simulation', 'day'],
    ascending=[True, True]
).reset_index(drop=True)

print(f"DataFrame created in {time.time() - df_start:.1f}s")

total_time = time.time() - total_start
print(f"\n" + "=" * 70)
print(f"SIMULATION COMPLETE! Total time: {total_time / 60:.1f} minutes")
print("=" * 70)

# =============================================================================
# STEP 6: DISPLAY AND SAVE
# =============================================================================

print("\n" + "=" * 70)
print("STEP 5: FINAL DATASET")
print("=" * 70)

print("\nFirst 20 rows:")
print(df.head(20))

print("\nLast 20 rows:")
print(df.tail(20))

print("\nStatistics:")
print(df.describe())

# Example simulations
for sim_id in [0, 1, 2]:
    sim_data = df[df['simulation'] == sim_id]
    print(f"\nSimulation {sim_id}:")
    print(f"  K assigned: £{sim_data['K'].iloc[0]:,.2f}")
    print(f"  Rows: {len(sim_data)}")
    print(f"  T range: {sim_data['T'].max():.4f} → {sim_data['T'].min():.4f}")
    print(f"  S range: £{sim_data['S'].min():.2f} → £{sim_data['S'].max():.2f}")

# Compute final hash
print("\nComputing dataset hash...")
final_hash = compute_dataframe_hash(df)
print(f"Final dataset hash: {final_hash}")

# Save final file as CSV
print("\nSaving final dataset...")
save_start = time.time()
df.to_csv('ftse_garch_simulation_data_T5.csv', index=False)
save_time = time.time() - save_start

file_size_gb = os.path.getsize('ftse_garch_simulation_data_T5.csv') / 1e9
print(f"Saved to ftse_garch_simulation_data_T5.csv in {save_time:.1f}s")
print(f"File size: {file_size_gb:.2f} GB")

# =============================================================================
# STEP 7: SAVE METADATA AND VERIFICATION INFO
# =============================================================================

print("\n" + "=" * 70)
print("STEP 6: SAVING METADATA FOR REPRODUCIBILITY")
print("=" * 70)

# Add hashes and final statistics to metadata
metadata['final_hash'] = final_hash
metadata['total_rows'] = int(len(df))
metadata['file_size_gb'] = float(file_size_gb)
metadata['total_generation_time_minutes'] = float(total_time / 60)
metadata['simulation_seed'] = int(simulation_seed)

# Save metadata
save_metadata(metadata, f'{output_dir}/dataset_metadata.json')

# Also save a verification file with just the hash
verification = {
    'master_seed': MASTER_SEED,
    'simulation_seed': simulation_seed,
    'final_hash': final_hash,
    'total_rows': int(len(df)),
    'n_simulations': int(n_simulations),
    'T_maturity': int(T_maturity),
    'n_days': int(n_days),
    'instructions': f'Run the generation script with MASTER_SEED={MASTER_SEED} to reproduce this exact dataset'
}

with open('REPRODUCIBILITY_INFO.json', 'w') as f:
    json.dump(verification, f, indent=2)

print(f"\nVerification info saved to REPRODUCIBILITY_INFO.json")

print("\n" + "=" * 70)
print("DONE!")
print("=" * 70)
print("\nTo reproduce this dataset on another computer:")
print(f"1. Use MASTER_SEED = {MASTER_SEED}")
print(f"2. Expected final hash: {final_hash}")
print(f"3. Expected total rows: {len(df):,}")
print(f"4. Expected file size: {file_size_gb:.2f} GB")
print("=" * 70)

# Clean up memory
del S_paths, sigma_paths
